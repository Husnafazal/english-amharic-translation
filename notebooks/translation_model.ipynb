{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9pfUdMngz_p"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install necessary packages"
      ],
      "metadata": {
        "id": "wAJzRXNEiRv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25ok2TC4iUlo",
        "outputId": "794f9be3-8a0f-44c7-ec46-d3adfc6a241f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import necessary libraries"
      ],
      "metadata": {
        "id": "wdvLXmWzibCU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import json\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Create English sentences file\n",
        "english_sentences = \"\"\"\n",
        "Hello world\n",
        "How are you?\n",
        "Good morning\n",
        "Thank you\n",
        "I love programming\n",
        "See you later\n",
        "What is your name?\n",
        "Have a great day\n",
        "Where is the nearest hospital?\n",
        "I need help\n",
        "\"\"\"\n",
        "!mkdir -p /content/data\n",
        "with open('/content/data/english_sentences.txt', 'w', encoding='utf-8') as f:\n",
        "    f.write(english_sentences.strip())\n",
        "\n",
        "# Create Amharic sentences file\n",
        "amharic_sentences = \"\"\"\n",
        "ሰላም ልዑል\n",
        "እንዴት ነህ?\n",
        "እንኳን ደህና አደርህ\n",
        "አመሰግናለሁ\n",
        "እኔ ፕሮግራሚንግ እወዳለሁ\n",
        "ኋላ እንገናኝ\n",
        "ስምህ ማን ነው?\n",
        "በጣም ጥሩ ቀን አለህ\n",
        "ቅርብ ሆስፒታል ወዴት ነው?\n",
        "እርዳታ ያስፈልገኛል\n",
        "\"\"\"\n",
        "\n",
        "with open('/content/data/amharic_sentences.txt', 'w', encoding='utf-8') as f:\n",
        "    f.write(amharic_sentences.strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4O_PKzpCid0D",
        "outputId": "4b1a3283-e259-4963-e91c-23159bb01217"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading"
      ],
      "metadata": {
        "id": "YACMRBK6isCR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        data = file.read().split('\\n')\n",
        "    return data\n"
      ],
      "metadata": {
        "id": "GPFOSbGFiqEn"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing\n"
      ],
      "metadata": {
        "id": "48ihbfvTi1qA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(data, max_len=None):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(data)\n",
        "    sequences = tokenizer.texts_to_sequences(data)\n",
        "    if not max_len:\n",
        "        max_len = max([len(seq) for seq in sequences])\n",
        "    padded_sequences = pad_sequences(sequences, padding='post', maxlen=max_len)\n",
        "    vocab_size = len(tokenizer.word_index) + 1\n",
        "    return padded_sequences, tokenizer, vocab_size, max_lensize, max_len"
      ],
      "metadata": {
        "id": "T-MuM3Jjiziq"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Building"
      ],
      "metadata": {
        "id": "Edjdl8h7i86A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(data, max_len=None):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(data)\n",
        "    sequences = tokenizer.texts_to_sequences(data)\n",
        "    if not max_len:\n",
        "        max_len = max([len(seq) for seq in sequences])\n",
        "    padded_sequences = pad_sequences(sequences, padding='post', maxlen=max_len)\n",
        "    vocab_size = len(tokenizer.word_index) + 1\n",
        "    return padded_sequences, tokenizer, vocab_size, max_len"
      ],
      "metadata": {
        "id": "XT1DakL7jEtK"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load datasets"
      ],
      "metadata": {
        "id": "wrCwnhxnjKk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "source_sentences = load_data('/content/data/english_sentences.txt')\n",
        "target_sentences = load_data('/content/data/amharic_sentences.txt')"
      ],
      "metadata": {
        "id": "ojAogVEpjNjn"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess datasets"
      ],
      "metadata": {
        "id": "9vYHn2uvlNg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "source_padded, source_tokenizer, source_vocab_size, source_max_len = preprocess_data(source_sentences)\n",
        "target_padded, target_tokenizer, target_vocab_size, target_max_len = preprocess_data(target_sentences)\n",
        "\n",
        "# Ensure both source and target sequences are padded to the same length\n",
        "max_len = max(source_max_len, target_max_len)\n",
        "source_padded = pad_sequences(source_padded, padding='post', maxlen=max_len)\n",
        "target_padded = pad_sequences(target_padded, padding='post', maxlen=max_len)"
      ],
      "metadata": {
        "id": "EPfZHIqHlSF6"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build and train model"
      ],
      "metadata": {
        "id": "OIedvlitlZud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 100\n",
        "model = build_model(source_vocab_size, target_vocab_size, embedding_dim, source_max_len)\n",
        "model.fit(source_padded, np.expand_dims(target_padded, -1), epochs=10, batch_size=32, validation_split=0.2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWx34792lYVA",
        "outputId": "40601c75-9d4d-4423-9e35-8714d5120e77"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1/1 [==============================] - 5s 5s/step - loss: 3.2641 - accuracy: 0.0250 - val_loss: 3.2520 - val_accuracy: 0.3000\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 3.2468 - accuracy: 0.3750 - val_loss: 3.2432 - val_accuracy: 0.4000\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 3.2295 - accuracy: 0.5000 - val_loss: 3.2342 - val_accuracy: 0.4000\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 3.2119 - accuracy: 0.5000 - val_loss: 3.2249 - val_accuracy: 0.4000\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 3.1936 - accuracy: 0.5000 - val_loss: 3.2151 - val_accuracy: 0.4000\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 3.1742 - accuracy: 0.5000 - val_loss: 3.2045 - val_accuracy: 0.4000\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 3.1533 - accuracy: 0.5000 - val_loss: 3.1931 - val_accuracy: 0.4000\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 3.1305 - accuracy: 0.5000 - val_loss: 3.1806 - val_accuracy: 0.4000\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 3.1053 - accuracy: 0.5000 - val_loss: 3.1668 - val_accuracy: 0.4000\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 3.0773 - accuracy: 0.5000 - val_loss: 3.1515 - val_accuracy: 0.4000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7bd97efa0fd0>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save model and tokenizers\n"
      ],
      "metadata": {
        "id": "_pg9KJwMmlTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('translation_model.h5')\n",
        "with open('source_tokenizer.json', 'w') as f:\n",
        "    f.write(source_tokenizer.to_json())\n",
        "with open('target_tokenizer.json', 'w') as f:\n",
        "    f.write(target_tokenizer.to_json())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4fxYNj7mqwH",
        "outputId": "091d5303-cdc5-45d7-b114-58df95b3fdca"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def evaluate_model(model, source_tokenizer, target_tokenizer, source_sentences, target_sentences, source_max_len):\n",
        "    source_sequences = source_tokenizer.texts_to_sequences(source_sentences)\n",
        "    source_padded = pad_sequences(source_sequences, maxlen=source_max_len, padding='post')\n",
        "\n",
        "    predictions = model.predict(source_padded)\n",
        "    for i in range(len(source_sentences)):\n",
        "        predicted_sequence = np.argmax(predictions[i], axis=-1)\n",
        "        predicted_sentence = ' '.join([target_tokenizer.index_word[idx] for idx in predicted_sequence if idx != 0])\n",
        "        print(f\"Original: {target_sentences[i]}\")\n",
        "        print(f\"Predicted: {predicted_sentence}\")\n",
        "        print(f\"BLEU score: {sentence_bleu([target_sentences[i].split()], predicted_sentence.split())}\")\n",
        "        print()\n"
      ],
      "metadata": {
        "id": "45tjKL_KmwKa"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(model, source_tokenizer, target_tokenizer, source_sentences, target_sentences, max_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8VfzAQWm5bL",
        "outputId": "c819a6bc-43b1-4a6b-c914-3296c4af6785"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 986ms/step\n",
            "Original: ሰላም ልዑል\n",
            "Predicted: \n",
            "BLEU score: 0\n",
            "\n",
            "Original: እንዴት ነህ?\n",
            "Predicted: \n",
            "BLEU score: 0\n",
            "\n",
            "Original: እንኳን ደህና አደርህ\n",
            "Predicted: \n",
            "BLEU score: 0\n",
            "\n",
            "Original: አመሰግናለሁ\n",
            "Predicted: \n",
            "BLEU score: 0\n",
            "\n",
            "Original: እኔ ፕሮግራሚንግ እወዳለሁ\n",
            "Predicted: \n",
            "BLEU score: 0\n",
            "\n",
            "Original: ኋላ እንገናኝ\n",
            "Predicted: \n",
            "BLEU score: 0\n",
            "\n",
            "Original: ስምህ ማን ነው?\n",
            "Predicted: \n",
            "BLEU score: 0\n",
            "\n",
            "Original: በጣም ጥሩ ቀን አለህ\n",
            "Predicted: \n",
            "BLEU score: 0\n",
            "\n",
            "Original: ቅርብ ሆስፒታል ወዴት ነው?\n",
            "Predicted: \n",
            "BLEU score: 0\n",
            "\n",
            "Original: እርዳታ ያስፈልገኛል\n",
            "Predicted: \n",
            "BLEU score: 0\n",
            "\n"
          ]
        }
      ]
    }
  ]
}